{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T14:31:15.920439Z",
     "start_time": "2019-08-17T14:31:15.908171Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "from multiprocessing import Pool, Value, Lock\n",
    "import openslide\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import pandas as pd\n",
    "import cv2\n",
    "# import multiresolutionimageinterface as mir\n",
    "from skimage.color import rgb2hsv\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import points_in_poly\n",
    "from skimage import feature\n",
    "from skimage.feature import canny\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "import tqdm\n",
    "from operator import itemgetter \n",
    "from collections import defaultdict\n",
    "np.random.seed(0)\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T10:47:17.708645Z",
     "start_time": "2019-08-15T10:47:17.700888Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# ROOT_PATH = '/media/balaji/CamelyonProject/CAMELYON_DATASET'\n",
    "ROOT_PATH = '/media/mak/mirlproject1'\n",
    "\n",
    "# Camelyon 2016\n",
    "train_tif_cm16_path = ROOT_PATH+'/CAMELYON16/TrainingData/normal_tumor'\n",
    "train_xml_cm16_path = ROOT_PATH+'/CAMELYON16/TrainingData/lesion_annotations'\n",
    "train_json_cm16_path = ROOT_PATH+'/CAMELYON16/TrainingData/lesion_annotations_jsons'\n",
    "train_mask_cm16_path = ROOT_PATH+'/CAMELYON16/TrainingData/lesion_masks'\n",
    "\n",
    "test_tif_cm16_path = ROOT_PATH+'/CAMELYON16/Testset/Images'\n",
    "test_xml_cm16_path = ROOT_PATH+'/CAMELYON16/Testset/lesion_annotations'\n",
    "test_json_cm16_path = ROOT_PATH+'/CAMELYON16/Testset/lesion_annotations_jsons'\n",
    "test_mask_cm16_path = ROOT_PATH+'/CAMELYON16/Testset/Backup/Masks/tif_files'\n",
    "\n",
    "# Camelyon 2017\n",
    "train_tif_cm17_path = ROOT_PATH+'/CAMELYON17/training/dataset'\n",
    "train_xml_cm17_path = ROOT_PATH+'/CAMELYON17/training/groundtruth/lesion_annotations/XML'\n",
    "train_json_cm17_path = ROOT_PATH+'/CAMELYON17/training/groundtruth/lesion_annotations/json'\n",
    "train_mask_cm17_path = ROOT_PATH+'/CAMELYON17/training/groundtruth/lesion_annotations/Mask'\n",
    "test_tif_cm17_path = ROOT_PATH+'/CAMELYON17/testing/centers/zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T06:41:40.118641Z",
     "start_time": "2019-08-15T06:41:40.113813Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Hardmined Points\n",
    "\n",
    "# CM16 NCRF points\n",
    "ncrf_train_hpoints = '/media/mak/Data/Projects/Camelyon17/code/keras_framework/patch_coords/hardmined_points/train_cm16_ncrf.txt'\n",
    "ncrf_valid_hpoints = '/media/mak/Data/Projects/Camelyon17/code/keras_framework/patch_coords/hardmined_points/valid_cm16_ncrf.txt'\n",
    "\n",
    "# CM17 hardmined points\n",
    "# Hardmined_Coordinates dir\n",
    "CM_17_hardmined_points_dir = '/media/mak/Data/Projects/Camelyon17/code/keras_framework/datasetgen/DenseNet-121_UNET_CM16_NCRF/Hardmine_CM17/level_5_16/csv'\n",
    "\n",
    "# CV fold: 3 folds exists\n",
    "fold_no = 2\n",
    "base_path = '/media/mak/Data/Projects/Camelyon17/code/keras_framework/datasetgen'\n",
    "# base_path = '/media/balaji/CamelyonProject/CAMELYON_DATASET/Projects/Semantic_Segmentation/datasetgen'\n",
    "CM_16_Train_train_split = base_path+'/cm16_train_cross_val_splits/training_fold_{}.csv'.format(fold_no)\n",
    "CM_16_Train_valid_split = base_path+'/cm16_train_cross_val_splits/validation_fold_{}.csv'.format(fold_no)\n",
    "\n",
    "CM_16_Test_train_split = base_path+'/cm16_test_cross_val_splits/training_fold_{}.csv'.format(fold_no)\n",
    "CM_16_Test_valid_split = base_path+'/cm16_test_cross_val_splits/validation_fold_{}.csv'.format(fold_no)\n",
    "\n",
    "CM_17_Train_train_split = base_path+'/cm17_cross_val_splits/training_fold_{}.csv'.format(fold_no)\n",
    "CM_17_Train_valid_split = base_path+'/cm17_cross_val_splits/validation_fold_{}.csv'.format(fold_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T06:41:40.963583Z",
     "start_time": "2019-08-15T06:41:40.943677Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Output path for text files of coordinates\n",
    "out_path = '/media/mak/Data/Projects/Camelyon17/code/keras_framework/patch_coords/cm17_16_train_test_ncrf_points_fold_{}'.format(fold_no)\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T14:31:17.603407Z",
     "start_time": "2019-08-17T14:31:17.532428Z"
    },
    "code_folding": [
     1,
     36,
     66,
     100,
     191,
     207,
     216,
     233,
     240,
     271,
     278,
     343
    ]
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def ReadWholeSlideImage(image_path, level=None, RGB=True, read_image=True):\n",
    "    \"\"\"\n",
    "    # =========================\n",
    "    # Read Whole-Slide Image \n",
    "    # =========================\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wsi_obj = openslide.OpenSlide(image_path)\n",
    "        n_levels = wsi_obj.level_count\n",
    "#         print(\"Number of Levels\", n_levels)\n",
    "#         print(\"Dimensions:%s, level_dimensions:%s\"%(wsi_obj.dimensions, wsi_obj.level_dimensions))\n",
    "#         print(\"Level_downsamples:\", wsi_obj.level_downsamples)        \n",
    "#         print(\"Properties\", wsi_obj.properties)     \n",
    "        if (level is None) or (level > n_levels-1):\n",
    "            level = n_levels-1\n",
    "#             print ('Default level selected', level)\n",
    "        if read_image:\n",
    "            if RGB:\n",
    "                image_data = np.transpose(np.array(wsi_obj.read_region((0, 0),\n",
    "                                   level,\n",
    "                                   wsi_obj.level_dimensions[level]).convert('RGB')),\n",
    "                                   axes=[1, 0, 2])\n",
    "            else: \n",
    "                image_data = np.array(wsi_obj.read_region((0, 0),\n",
    "                           level,\n",
    "                           wsi_obj.level_dimensions[level]).convert('L')).T\n",
    "        else:\n",
    "            image_data = None \n",
    "#         print (image_data.shape)\n",
    "    except openslide.OpenSlideUnsupportedFormatError:\n",
    "        print('Exception: OpenSlideUnsupportedFormatError')\n",
    "        return None, None, None\n",
    "\n",
    "    return wsi_obj, image_data, level\n",
    "\n",
    "def imshow(*args,**kwargs):\n",
    "    \"\"\" Handy function to show multiple plots in on row, possibly with different cmaps and titles\n",
    "    Usage:\n",
    "    imshow(img1, title=\"myPlot\")\n",
    "    imshow(img1,img2, title=['title1','title2'])\n",
    "    imshow(img1,img2, cmap='hot')\n",
    "    imshow(img1,img2,cmap=['gray','Blues']) \"\"\"\n",
    "    cmap = kwargs.get('cmap', 'gray')\n",
    "    title= kwargs.get('title','')\n",
    "    axis_off = kwargs.get('axis_off','')\n",
    "    if len(args)==0:\n",
    "        raise ValueError(\"No images given to imshow\")\n",
    "    elif len(args)==1:\n",
    "        plt.title(title)\n",
    "        plt.imshow(args[0], interpolation='none')\n",
    "    else:\n",
    "        n=len(args)\n",
    "        if type(cmap)==str:\n",
    "            cmap = [cmap]*n\n",
    "        if type(title)==str:\n",
    "            title= [title]*n\n",
    "        plt.figure(figsize=(n*5,10))\n",
    "        for i in range(n):\n",
    "            plt.subplot(1,n,i+1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(args[i], cmap[i])\n",
    "            if axis_off: \n",
    "              plt.axis('off')  \n",
    "    plt.show()\n",
    "    \n",
    "class Polygon(object):\n",
    "    \"\"\"\n",
    "    Polygon represented as [N, 2] array of vertices\n",
    "    \"\"\"\n",
    "    def __init__(self, name, vertices):\n",
    "        \"\"\"\n",
    "        Initialize the polygon.\n",
    "\n",
    "        Arguments:\n",
    "            name: string, name of the polygon\n",
    "            vertices: [N, 2] 2D numpy array of int\n",
    "        \"\"\"\n",
    "        self._name = name\n",
    "        self._vertices = vertices\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._name\n",
    "\n",
    "    def inside(self, coord):\n",
    "        \"\"\"\n",
    "        Determine if a given coordinate is inside the polygon or not.\n",
    "\n",
    "        Arguments:\n",
    "            coord: 2 element tuple of int, e.g. (x, y)\n",
    "\n",
    "        Returns:\n",
    "            bool, if the coord is inside the polygon.\n",
    "        \"\"\"\n",
    "        return points_in_poly([coord], self._vertices)[0]\n",
    "\n",
    "    def vertices(self):\n",
    "\n",
    "        return np.array(self._vertices)\n",
    "\n",
    "class Annotation(object):\n",
    "    \"\"\"\n",
    "    Annotation about the regions within BBOX in terms of vertices of polygons.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._bbox = []\n",
    "        self._polygons_positive = []\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._json_path\n",
    "\n",
    "    def from_json(self, json_path):\n",
    "        \"\"\"\n",
    "        Initialize the annotation from a json file.\n",
    "\n",
    "        Arguments:\n",
    "            json_path: string, path to the json annotation.\n",
    "        \"\"\"\n",
    "        self._json_path = json_path\n",
    "        with open(json_path) as f:\n",
    "            annotations_json = json.load(f)\n",
    "\n",
    "        for annotation in annotations_json['positive']:\n",
    "            name = annotation['name']\n",
    "            vertices = np.array(annotation['vertices'])      \n",
    "            polygon = Polygon(name, vertices)\n",
    "            if name == 'BBOX':\n",
    "                self._bbox.append(polygon)\n",
    "            else:\n",
    "                self._polygons_positive.append(polygon)\n",
    "                \n",
    "    def inside_bbox(self, coord):\n",
    "        \"\"\"\n",
    "        Determine if a given coordinate is inside the positive polygons of the annotation.\n",
    "\n",
    "        Arguments:\n",
    "            coord: 2 element tuple of int, e.g. (x, y)\n",
    "\n",
    "        Returns:\n",
    "            bool, if the coord is inside the positive/negative polygons of the\n",
    "            annotation.\n",
    "        \"\"\"\n",
    "        bboxes = copy.deepcopy(self._bbox)\n",
    "        for bbox in bboxes:\n",
    "            if bbox.inside(coord):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def bbox_vertices(self):\n",
    "        \"\"\"\n",
    "        Return the polygon represented as [N, 2] array of vertices\n",
    "\n",
    "        Arguments:\n",
    "            is_positive: bool, return positive or negative polygons.\n",
    "\n",
    "        Returns:\n",
    "            [N, 2] 2D array of int\n",
    "        \"\"\"\n",
    "        return list(map(lambda x: x.vertices(), self._bbox))\n",
    "    \n",
    "    def inside_polygons(self, coord):\n",
    "        \"\"\"\n",
    "        Determine if a given coordinate is inside the positive polygons of the annotation.\n",
    "\n",
    "        Arguments:\n",
    "            coord: 2 element tuple of int, e.g. (x, y)\n",
    "\n",
    "        Returns:\n",
    "            bool, if the coord is inside the positive/negative polygons of the\n",
    "            annotation.\n",
    "        \"\"\"\n",
    "        polygons = copy.deepcopy(self._polygons_positive)\n",
    "        \n",
    "        for polygon in polygons:\n",
    "            if polygon.inside(coord):\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def polygon_vertices(self):\n",
    "        \"\"\"\n",
    "        Return the polygon represented as [N, 2] array of vertices\n",
    "\n",
    "        Arguments:\n",
    "            is_positive: bool, return positive or negative polygons.\n",
    "\n",
    "        Returns:\n",
    "            [N, 2] 2D array of int\n",
    "        \"\"\"\n",
    "        return list(map(lambda x: x.vertices(), self._polygons_positive))\n",
    "    \n",
    "def TissueMask(img_RGB, level):\n",
    "    RGB_min = 50\n",
    "    # note the shape of img_RGB is the transpose of slide.level_dimensions\n",
    "    img_HSV = rgb2hsv(img_RGB)\n",
    "\n",
    "    background_R = img_RGB[:, :, 0] > threshold_otsu(img_RGB[:, :, 0])\n",
    "    background_G = img_RGB[:, :, 1] > threshold_otsu(img_RGB[:, :, 1])\n",
    "    background_B = img_RGB[:, :, 2] > threshold_otsu(img_RGB[:, :, 2])\n",
    "    tissue_RGB = np.logical_not(background_R & background_G & background_B)\n",
    "    tissue_S = img_HSV[:, :, 1] > threshold_otsu(img_HSV[:, :, 1])\n",
    "    min_R = img_RGB[:, :, 0] > RGB_min\n",
    "    min_G = img_RGB[:, :, 1] > RGB_min\n",
    "    min_B = img_RGB[:, :, 2] > RGB_min\n",
    "    tissue_mask = tissue_S & tissue_RGB & min_R & min_G & min_B\n",
    "    return tissue_mask\n",
    "\n",
    "def ShuffleAndSampleFirstN(data, n=10):\n",
    "    \"\"\"\n",
    "    Sampling by shuffling the data, then get only the first n elements.\";\n",
    "    \"\"\"\n",
    "    data=copy.deepcopy(data);\n",
    "    random.shuffle(data);\n",
    "    sample=data[0:n];\n",
    "    return sample\n",
    "\n",
    "def RandomUniformSample(data, n=1000, factor=1):\n",
    "    data=copy.deepcopy(data);\n",
    "    if len(data) <= n:\n",
    "        sample_n = len(data)*factor        \n",
    "    else:\n",
    "        sample_n = n\n",
    "        \n",
    "    idxs = [];\n",
    "    while len(idxs)<sample_n:\n",
    "        rand=int(random.uniform(0, len(data)))\n",
    "        if rand in idxs:\n",
    "            pass\n",
    "        else:\n",
    "            idxs.append(rand);\n",
    "    sample=[data[i] for i in idxs];\n",
    "    return sample\n",
    "\n",
    "def merge_files(file_list, output_file_path):\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for fname in file_list:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line) \n",
    "                    \n",
    "def combine_text_files(files_dir_path, data_split_csv, output_file):\n",
    "    \"\"\"\n",
    "    Combine all the files listed in data_split_csv from \"files_dir_path\" for CM17 dataset\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    data_split_df = pd.read_csv(data_split_csv)\n",
    "    for i in range(len(data_split_df.Image_Path)):\n",
    "        file_path = os.path.join(files_dir_path, os.path.basename(data_split_df.Image_Path[i]).split('.')[0])\n",
    "        files.append(file_path)\n",
    "    mask_files = []\n",
    "    for i in range(len(data_split_df.Mask_Path)):\n",
    "        if data_split_df.Mask_Path[i] !='0':\n",
    "            mask_dir = os.path.dirname(data_split_df.Mask_Path[i])\n",
    "            mask_files.append(os.path.basename(data_split_df.Mask_Path[i]))\n",
    "    image_dir = os.path.dirname(os.path.dirname(data_split_df.Image_Path[i]))            \n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for fname in files:\n",
    "            with open(fname) as infile:\n",
    "                for line in infile:\n",
    "                    pid, x_center, y_center = line.strip('\\n').split(',')[0:3]\n",
    "                    pid_no = int(pid.split('_')[1])\n",
    "                    center_folder = 'center_'+str(int(pid_no//20))\n",
    "                    pid_path = os.path.join(image_dir,center_folder,pid)\n",
    "                    mask_name = pid.split('.')[0]+'_mask.tif'\n",
    "                    if mask_name in mask_files:\n",
    "                        mask_path = os.path.join(mask_dir, pid.split('.')[0]+'_mask.tif')\n",
    "                        line = pid_path+','+mask_path+','+x_center+','+y_center+'\\n'\n",
    "                    else:\n",
    "                        line = pid_path+','+str(0)+','+x_center+','+y_center+'\\n'\n",
    "                    outfile.write(line) \n",
    "                    \n",
    "def threshold_img(img):\n",
    "    '''\n",
    "    Transforms a numpy array such that values greater than 0 are converted to 255\n",
    "    '''\n",
    "    img = np.array(img)\n",
    "    np.place(img,img>0,255)\n",
    "    return img\n",
    "def extract_normal_patches_from_wsi(image_path, mask_path, json_path, out_path, mode, max_normal_points=1000):                    \n",
    "    '''\n",
    "    Extract Normal Patches coordinates and write to text file\n",
    "    '''\n",
    "    print('Extracting normal patches for %s' %(os.path.basename(image_path)))\n",
    "    patch_level = 0\n",
    "    patch_size = 256\n",
    "    tumor_threshold = 0\n",
    "    img_sampling_level = 2\n",
    "    #Img downsamples are pows of 4, mask downsamples are pows of 2\n",
    "    mask_sampling_level = int(math.sqrt(pow(4,img_sampling_level)))\n",
    "    target_file = open(os.path.join(out_path, \"{}_random_sample.txt\".format(mode)), 'a')\n",
    "    \n",
    "    if os.path.exists(mask_path):\n",
    "        print('True condition')\n",
    "        wsi_obj, img_data, level = ReadWholeSlideImage(image_path, img_sampling_level, read_image=True)   \n",
    "        mask_obj, mask_data, level = ReadWholeSlideImage(mask_path, mask_sampling_level)\n",
    "#         if sampling_level > level:\n",
    "#             sampling_level = level\n",
    "        tissue_mask = TissueMask(img_data, img_sampling_level)\n",
    "#         imshow(tissue_mask,threshold_img(mask_data))\n",
    "        sampled_normal_pixels = np.transpose(np.nonzero(tissue_mask))\n",
    "        \n",
    "        # Perform Uniform sampling\n",
    "        sampled_normal_pixels = RandomUniformSample(sampled_normal_pixels, 2*max_normal_points)\n",
    "        sampled_normal_pixels_verified = []\n",
    "        org_mag_factor = pow(4, img_sampling_level)                \n",
    "        for coord in sampled_normal_pixels:   \n",
    "            scoord = (int(coord[0]*org_mag_factor), int(coord[1]*org_mag_factor))\n",
    "            shifted_point = (int(scoord[0]-patch_size//2), int(scoord[1]-patch_size//2))\n",
    "            mask_patch = np.array(mask_obj.read_region(shifted_point, patch_level, (patch_size, patch_size)).convert('L'))        \n",
    "            tumor_fraction = np.count_nonzero(mask_patch)/np.prod(mask_patch.shape) \n",
    "            if tumor_fraction <= tumor_threshold:\n",
    "                sampled_normal_pixels_verified.append(scoord)\n",
    "                slide_patch = np.array(wsi_obj.read_region(shifted_point, patch_level, (patch_size, patch_size)).convert('RGB'))\n",
    "#                 imshow(slide_patch, mask_patch)\n",
    "    else:\n",
    "        print('False condition')\n",
    "        mask_path = '0'        \n",
    "        wsi_obj, img_data, level = ReadWholeSlideImage(image_path, sampling_level, read_image=True)   \n",
    "        if sampling_level > level:\n",
    "            sampling_level = level        \n",
    "        tissue_mask = TissueMask(img_data, sampling_level)\n",
    "#         imshow(tissue_mask)\n",
    "        sampled_normal_pixels = list(np.transpose(np.nonzero(tissue_mask)))\n",
    "        sampled_normal_pixels_verified = []\n",
    "        org_mag_factor = pow(4, sampling_level)    \n",
    "        for coord in sampled_normal_pixels:   \n",
    "            scoord = (int(coord[0]*org_mag_factor), int(coord[1]*org_mag_factor))   \n",
    "            sampled_normal_pixels_verified.append(scoord)\n",
    "#         for coord in sampled_normal_pixels_verified:   \n",
    "#             scaled_shifted_point = (int(coord[0]-patch_size//2), int(coord[1]-patch_size//2))\n",
    "#             slide_patch = np.array(wsi_obj.read_region(scaled_shifted_point, patch_level, (patch_size, patch_size)).convert('RGB'))\n",
    "#             imshow(slide_patch)\n",
    "        \n",
    "    # Perform Uniform sampling\n",
    "    sampled_normal_pixels_verified = RandomUniformSample(sampled_normal_pixels_verified, max_normal_points)    \n",
    "    for tpoint in sampled_normal_pixels_verified:\n",
    "        target_file.write(image_path +','+mask_path +','+ str(tpoint[0]) + ',' + str(tpoint[1]))        \n",
    "        target_file.write(\"\\n\")\n",
    "    target_file.close()    \n",
    "    no_samples = (len(sampled_normal_pixels_verified))                    \n",
    "    print('Extracted %d normal samples' % (no_samples))\n",
    "    return no_samples\n",
    "                  \n",
    "def extract_tumor_patches_from_wsi(image_path, mask_path, json_path, out_path, mode, max_tumor_points=2500):\n",
    "    '''\n",
    "    Extract Patches coordinates and write to text file\n",
    "    '''\n",
    "    print('Extracting tumor patches for %s' %(os.path.basename(image_path)))\n",
    "    patch_size = 256\n",
    "    patch_level = 0\n",
    "    img_sampling_level = 2\n",
    "    #Img downsamples are pows of 4, mask downsamples are pows of 2\n",
    "    mask_sampling_level = int(math.sqrt(pow(4,img_sampling_level)))\n",
    "    \n",
    "    target_file = open(os.path.join(out_path, \"{}_random_sample.txt\".format(mode)), 'a')\n",
    "    mask_obj, mask_data, level = ReadWholeSlideImage(mask_path, mask_sampling_level, RGB=False, read_image=True)\n",
    "    org_mag_factor = pow(4, img_sampling_level)\n",
    "    tumor_pixels = list(np.transpose(np.nonzero(mask_data)))\n",
    "    tumor_pixels = RandomUniformSample(tumor_pixels, max_tumor_points) \n",
    "#     anno = Annotation()\n",
    "#     anno.from_json(json_path)  \n",
    "#     anno_vertices_list = list(anno.polygon_vertices())\n",
    "#     anno_vertices_flat_list = [item for sublist in anno_vertices_list for item in sublist]\n",
    "#     sampled_anno_vertices_flat_list = RandomUniformSample(anno_vertices_flat_list, max_tumor_points)        \n",
    "    \n",
    "    # Perform Uniform sampling    \n",
    "    scaled_tumor_pixels = []\n",
    "    for coord in list(tumor_pixels):    \n",
    "        scoord = (int(coord[0]*org_mag_factor), int(coord[1]*org_mag_factor))   \n",
    "        scaled_tumor_pixels.append(scoord)\n",
    "                   \n",
    "#     print ('Number of Tumor pixels', len(scaled_tumor_pixels))\n",
    "#     scaled_tumor_pixels.extend(sampled_anno_vertices_flat_list)    \n",
    "#     print ('Number of Tumor pixels+ vertices', len(scaled_tumor_pixels))\n",
    "    \n",
    "#     for coord in scaled_tumor_pixels:\n",
    "#         print (coord)\n",
    "#         scaled_shifted_point = (coord[0]-patch_size//2, coord[1]-patch_size//2)\n",
    "#         wsi_obj, _, level = ReadWholeSlideImage(image_path, img_sampling_level, RGB=True, read_image=False)\n",
    "#         slide_patch = np.array(wsi_obj.read_region(scaled_shifted_point, patch_level, (patch_size, patch_size)).convert('RGB'))\n",
    "#         mask_patch = threshold_img(np.array(mask_obj.read_region(scaled_shifted_point, patch_level, (patch_size, patch_size)).convert('L')))\n",
    "#         imshow(slide_patch, mask_patch)  \n",
    "                \n",
    "    for tpoint in scaled_tumor_pixels:\n",
    "#         target_file.write(os.path.basename(image_path) +','+ str(tpoint[0]) + ',' + str(tpoint[1]))\n",
    "        target_file.write(image_path +','+mask_path +','+ str(tpoint[0]) + ',' + str(tpoint[1]))        \n",
    "        target_file.write(\"\\n\")\n",
    "\n",
    "    target_file.close()\n",
    "    no_samples = (len(scaled_tumor_pixels))\n",
    "    print('Extracted %d tumor samples' % (no_samples))\n",
    "    return no_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T14:31:17.926683Z",
     "start_time": "2019-08-17T14:31:17.871938Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "train_n_patches = 1000\n",
    "train_t_patches = 2500\n",
    "total_train = train_n_patches+train_t_patches\n",
    "print(total_train*n_samples)\n",
    "\n",
    "valid_n_patches = train_n_patches//5\n",
    "valid_t_patches = train_t_patches//5\n",
    "total_valid = valid_n_patches+valid_t_patches\n",
    "print(total_valid*n_samples)\n",
    "\n",
    "data_path = os.path.join('..','..','data','raw-data','train')\n",
    "out_path = os.path.join(data_path,'..','patch_coords_%dk'%(total_train*n_samples//1000))\n",
    "if not os.path.isdir(out_path):\n",
    "    os.makedirs(out_path)\n",
    "ids = os.listdir(data_path)\n",
    "\n",
    "def batch_patch_gen(mode,tumor_type):\n",
    "    count = 0\n",
    "    if mode == 'train':\n",
    "        n_patches = train_n_patches\n",
    "        t_patches = train_t_patches\n",
    "    elif mode == 'valid':\n",
    "        n_patches = valid_n_patches\n",
    "        t_patches = valid_t_patches\n",
    "    else:\n",
    "        return 0\n",
    "    mode = '%s_paip_%s' % (mode,tumor_type)\n",
    "    glob_str = '*%s*.tiff' % (tumor_type)\n",
    "    for i,id in enumerate(ids):\n",
    "        print('%d/%d : %s' %(i+1,len(ids),id))\n",
    "        image_path = glob.glob(os.path.join(data_path,id,'*.svs'))[0]\n",
    "        mask_path = glob.glob(os.path.join(data_path,id,glob_str))[0]\n",
    "        abspath = os.path.abspath\n",
    "        image_path = abspath(image_path)\n",
    "        mask_path = abspath(mask_path)\n",
    "        count+=extract_normal_patches_from_wsi(image_path, mask_path, None, out_path, mode,n_patches)\n",
    "        if os.path.exists(mask_path):\n",
    "            count+=extract_tumor_patches_from_wsi(image_path, mask_path, None, out_path, mode,t_patches)\n",
    "    print ('Points sampled:', train_count)\n",
    "    return '%s_paip_%s_random_sample.txt' % (mode,tumor_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T14:31:29.150197Z",
     "start_time": "2019-08-17T14:31:25.542619Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_patch_gen('train','whole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:45:53.674938Z",
     "start_time": "2019-08-16T10:09:15.549797Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_patch_gen('valid','whole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# CM17 Train Random Sample Coordinates\n",
    "mode = 'train_CM17_Train'\n",
    "train_split_df = pd.read_csv(CM_17_Train_train_split)\n",
    "for index, row in train_split_df.iterrows():\n",
    "    image_path = row['Image_Path']\n",
    "    image_file = os.path.basename(image_path).split('.')[0]\n",
    "#     print (image_file)\n",
    "    mask_path = os.path.join(train_mask_cm17_path, image_file +'_mask.tif')\n",
    "    json_path = os.path.join(train_json_cm17_path, image_file +'.json')\n",
    "    train_count+=extract_normal_patches_from_wsi(image_path, mask_path, json_path, out_path, mode)    \n",
    "    if os.path.exists(mask_path):\n",
    "        train_count+=extract_tumor_patches_from_wsi(image_path, mask_path, json_path, out_path, mode, max_tumor_points=25000)\n",
    "print ('Points sampled:', train_count)\n",
    "\n",
    "mode = 'valid_CM17_Train'    \n",
    "valid_split_df = pd.read_csv(CM_17_Train_valid_split)\n",
    "for index, row in valid_split_df.iterrows():\n",
    "    image_path = row['Image_Path']\n",
    "    image_file = os.path.basename(image_path).split('.')[0]\n",
    "    mask_path = os.path.join(train_mask_cm17_path, image_file+'_mask.tif')\n",
    "    valid_count+=extract_normal_patches_from_wsi(image_path, mask_path, json_path, out_path, mode)\n",
    "    if os.path.exists(mask_path):    \n",
    "#         print (mask_path)\n",
    "        valid_count+=extract_tumor_patches_from_wsi(image_path, mask_path, json_path, out_path, mode, max_tumor_points=25000)\n",
    "print ('Points sampled:', valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get Hardmined points from CM17 training dataset from CM16 trained model\n",
    "# Hardmined from CM16 training on CM17 training dataset with annotation\n",
    "cm17_train_hpoints = os.path.join(out_path, 'train_CM17_Hardmined_CM16_Model.txt')\n",
    "cm17_valid_hpoints = os.path.join(out_path, 'valid_CM17_Hardmined_CM16_Model.txt')\n",
    "\n",
    "combine_text_files(CM_17_hardmined_points_dir, CM_17_Train_train_split, cm17_train_hpoints)\n",
    "train_count+= sum(1 for line in open(cm17_train_hpoints))\n",
    "print ('Points sampled:', train_count)\n",
    "combine_text_files(CM_17_hardmined_points_dir, CM_17_Train_valid_split, cm17_valid_hpoints)\n",
    "valid_count+= sum(1 for line in open(cm17_valid_hpoints))\n",
    "print ('Points sampled:', valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T14:52:09.463657Z",
     "start_time": "2019-08-17T14:31:34.699309Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize(coord_file_path, patch_size=(256,256)):\n",
    "    tumor_samples = 0\n",
    "    fi = open(coord_file_path)\n",
    "    for i, line in enumerate(fi):\n",
    "        image_path, mask_path, x_center, y_center = line.strip('\\n').split(',')[0:4]\n",
    "        #print('%d %s'%(i,mask_path))\n",
    "        x_top_left = int(int(x_center) - patch_size[0] / 2)\n",
    "        y_top_left = int(int(y_center) - patch_size[1] / 2)            \n",
    "        image_opslide = openslide.OpenSlide(image_path)\n",
    "        image_data = image_opslide.read_region(\n",
    "            (x_top_left, y_top_left), 0,\n",
    "            patch_size).convert('RGB')        \n",
    "        if mask_path != '0':                       \n",
    "            x_top_left = int(int(x_center) - patch_size[0] / 2)\n",
    "            y_top_left = int(int(y_center) - patch_size[1] / 2)            \n",
    "            mask_obj = openslide.OpenSlide(mask_path)                                   \n",
    "            mask_data = np.array(mask_obj.read_region((x_top_left, y_top_left),\n",
    "                               0,\n",
    "                               patch_size).convert('L'))\n",
    "            np.place(mask_data,mask_data>0,255)\n",
    "            fraction = np.count_nonzero(mask_data)/np.prod(mask_data.shape)\n",
    "            if fraction > 0.0:\n",
    "                imshow(image_data, mask_data)                   \n",
    "        else:\n",
    "            mask_data = np.zeros_like(image_data)\n",
    "     \n",
    "        if not i%1000:\n",
    "            print(i)\n",
    "            imshow(image_data, mask_data)\n",
    "    fi.close()\n",
    "\n",
    "#visualize(os.path.join(out_path,'train_paip_whole_random_sample.txt')) \n",
    "visualize('/media/brats/mirlproject2/haranrk/paip-2019/data/raw-data/patch_coords_80k/3fold_0/training_whole_normal.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-17T15:01:41.957948Z",
     "start_time": "2019-08-17T14:52:09.466327Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize('/media/brats/mirlproject2/haranrk/paip-2019/data/raw-data/patch_coords_80k/3fold_0/training_whole_tumor.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# CM16 NCRF coordinates\n",
    "ncrf_patient_dict = defaultdict(list)\n",
    "with open(ncrf_train_hpoints) as infile:\n",
    "    for line in infile:\n",
    "        pid, x_center, y_center = line.strip('\\n').split(',')[0:3]\n",
    "        ncrf_patient_dict[pid].append([x_center, y_center])\n",
    "\n",
    "with open(ncrf_valid_hpoints) as infile:\n",
    "    for line in infile:\n",
    "        pid, x_center, y_center = line.strip('\\n').split(',')[0:3]\n",
    "        ncrf_patient_dict[pid].append([x_center, y_center])\n",
    "\n",
    "mode = 'train_CM16_Train_NCRF'\n",
    "train_split_df = pd.read_csv(CM_16_Train_train_split)\n",
    "for index, row in train_split_df.iterrows():\n",
    "    image_path = row['Image_Path']\n",
    "    image_file = os.path.basename(image_path).split('.')[0]\n",
    "    if len(ncrf_patient_dict[image_file])!=0:\n",
    "        target_file = open(os.path.join(out_path, \"{}.txt\".format(mode)), 'a')\n",
    "        mask_path = os.path.join(train_mask_cm16_path, image_file +'_Mask.tif')\n",
    "        if not os.path.exists(mask_path):\n",
    "            mask_path = str(0)\n",
    "        for tpoint in ncrf_patient_dict[image_file]:\n",
    "            target_file.write(image_path +','+mask_path +','+ str(tpoint[0]) + ',' + str(tpoint[1]))        \n",
    "            target_file.write(\"\\n\")\n",
    "train_count+= sum(1 for line in open(os.path.join(out_path, \"{}.txt\".format(mode))))\n",
    "print ('Points sampled:', train_count)        \n",
    "\n",
    "mode = 'valid_CM16_Train_NCRF'    \n",
    "valid_split_df = pd.read_csv(CM_16_Train_valid_split)\n",
    "for index, row in valid_split_df.iterrows():\n",
    "    image_path = row['Image_Path']\n",
    "    image_file = os.path.basename(image_path).split('.')[0]\n",
    "    if len(ncrf_patient_dict[image_file])!=0:\n",
    "        target_file = open(os.path.join(out_path, \"{}.txt\".format(mode)), 'a')\n",
    "        mask_path = os.path.join(train_mask_cm16_path, image_file +'_Mask.tif')\n",
    "        if not os.path.exists(mask_path):\n",
    "            mask_path = str(0)\n",
    "        for tpoint in ncrf_patient_dict[image_file]:\n",
    "            target_file.write(image_path +','+mask_path +','+ str(tpoint[0]) + ',' + str(tpoint[1]))        \n",
    "            target_file.write(\"\\n\")\n",
    "valid_count+= sum(1 for line in open(os.path.join(out_path, \"{}.txt\".format(mode))))\n",
    "print ('Points sampled:', valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Combine all text files:\n",
    "train_coord_file_list = glob.glob(out_path+'/train_*')\n",
    "valid_coord_file_list = glob.glob(out_path+'/valid_*')\n",
    "train_cm17 = os.path.join(out_path, 'train.txt')\n",
    "valid_cm17 = os.path.join(out_path, 'valid.txt')\n",
    "\n",
    "train_count = 0\n",
    "valid_count = 0\n",
    "\n",
    "merge_files(train_coord_file_list, train_cm17)\n",
    "merge_files(valid_coord_file_list, valid_cm17)\n",
    "train_count+= sum(1 for line in open(train_cm17))\n",
    "print ('Points sampled:', train_count)\n",
    "valid_count+= sum(1 for line in open(valid_cm17))\n",
    "print ('Points sampled:', valid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T11:46:15.618487Z",
     "start_time": "2019-08-16T11:46:15.595931Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_tumor_fraction(mask_image):\n",
    "    fraction = np.count_nonzero(mask_image)/np.prod(mask_image.shape)\n",
    "    return fraction\n",
    "                                   \n",
    "def add_tumor_fraction(coord_file_path, out_file_name, patch_size=(768,768)):\n",
    "    tumor_samples = 0\n",
    "    fi = open(coord_file_path)\n",
    "    fo = open(os.path.dirname(coord_file_path)+'/'+ out_file_name, 'a')  \n",
    "    for i,line in enumerate(fi):\n",
    "        image_path, mask_path, x_center, y_center = line.strip('\\n').split(',')[0:4]\n",
    "        if mask_path != '0':                       \n",
    "            x_top_left = int(int(x_center) - patch_size[0] / 2)\n",
    "            y_top_left = int(int(y_center) - patch_size[1] / 2)            \n",
    "            mask_obj = openslide.OpenSlide(mask_path)                                   \n",
    "            mask_data = np.array(mask_obj.read_region((x_top_left, y_top_left),\n",
    "                               0,\n",
    "                               patch_size).convert('L'))                                   \n",
    "            tumor_fraction = get_tumor_fraction(mask_data)\n",
    "            if tumor_fraction > 0.0:\n",
    "                tumor_samples += 1\n",
    "#                 image_opslide = openslide.OpenSlide(image_path)\n",
    "#                 image_data = image_opslide.read_region(\n",
    "#                     (x_top_left, y_top_left), 0,\n",
    "#                     patch_size).convert('RGB')\n",
    "#                 print (mask_path, tumor_fraction)\n",
    "#                 imshow(image_data, mask_data)\n",
    "        else:\n",
    "            tumor_fraction = 0\n",
    "        fo.write(image_path +','+mask_path +','+x_center+','+y_center+','+str(tumor_fraction))        \n",
    "        fo.write(\"\\n\")\n",
    "    fo.close()\n",
    "    fi.close()\n",
    "    return tumor_samples\n",
    "\n",
    "def wrapper_for_tumor_fraction(mode,tumor_type):\n",
    "    train_coord_path = os.path.join(out_path,'%s_paip_%s_random_sample.txt' % (mode,tumor_type))\n",
    "    train_tumor_count = add_tumor_fraction(train_coord_path, '%s_%s_tf.txt' % (mode,tumor_type))\n",
    "    print ('Train Stats:', 'Tumor_samples:', train_tumor_count, 'Normal_samples:', (train_count - train_tumor_count))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:34:57.864993Z",
     "start_time": "2019-08-16T11:46:15.941794Z"
    }
   },
   "outputs": [],
   "source": [
    "wrapper_for_tumor_fraction('train','whole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:06:48.723833Z",
     "start_time": "2019-08-16T04:32:05.472148Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_coord_path = os.path.join(out_path,'train_paip_tiss_whole_random_sample.txt')\n",
    "train_tumor_count = add_tumor_fraction(train_coord_path, 'train_tf.txt')\n",
    "print ('Train Stats:', 'Tumor_samples:', train_tumor_count, 'Normal_samples:', (train_count - train_tumor_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T07:18:37.332612Z",
     "start_time": "2019-08-16T06:32:21.582857Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "valid_coord_path = os.path.join(out_path,'valid_paip_tiss_whole_random_sample.txt')\n",
    "valid_tumor_count = add_tumor_fraction(valid_coord_path, 'valid_tf.txt')\n",
    "print ('Valid Stats:', 'Tumor_samples:', valid_tumor_count, 'Normal_samples:', (valid_count - valid_tumor_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T13:34:57.866780Z",
     "start_time": "2019-08-16T11:53:39.382Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_df(df, column, save_dir, mode,threshold=0):\n",
    "    df_tumor = df.loc[df[column]>threshold]\n",
    "    df_normal = df.loc[df[column]==threshold]\n",
    "    df_tumor.to_csv(os.path.join(save_dir,'{}_tumor.txt'.format(mode)), header=False, index=False)\n",
    "    df_normal.to_csv(os.path.join(save_dir,'{}_normal.txt'.format(mode)), header=False, index=False)    \n",
    "    return(df_tumor, df_normal)\n",
    "\n",
    "def split_df_wrapper(mode,tumor_type):\n",
    "    train_cm17_tf = os.path.join(out_path,'%s_paip_%s_random_sample.txt' % (mode,tumor_type))'train_tf.txt')\n",
    "    train_cm17_tf_df = pd.read_csv(train_cm17_tf, names=['pid', 'mask', 'x', 'y', 'tf'])\n",
    "    train_df_tumor, train_df_normal = split_df(train_cm17_tf_df, 'tf', out_path, '%s_%s'&(mode,tumor_type))\n",
    "    print (len(train_df_tumor), len(train_df_normal))\n",
    "\n",
    "split_df_wrapper('train','tumor_type')\n",
    "    \n",
    "# Split the dataset into tumor and normal cases\n",
    "# train_cm17_tf = os.path.join(out_path, 'train_tf.txt')\n",
    "# valid_cm17_tf = os.path.join(out_path, 'valid_tf.txt')\n",
    "\n",
    "# train_cm17_tf_df = pd.read_csv(train_cm17_tf, names=['pid', 'mask', 'x', 'y', 'tf'])\n",
    "# valid_cm17_tf_df = pd.read_csv(valid_cm17_tf, names=['pid', 'mask', 'x', 'y', 'tf'])\n",
    "\n",
    "# train_df_tumor, train_df_normal = split_df(train_cm17_tf_df, 'tf', out_path, 'train')\n",
    "# print (len(train_df_tumor), len(train_df_normal))\n",
    "# valid_df_tumor, valid_df_normal = split_df(valid_cm17_tf_df, 'tf', out_path, 'valid')\n",
    "# print (len(valid_df_tumor), len(valid_df_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Path_Changer and to generate cross_val_folds \n",
    "wsi_dict = defaultdict(list)\n",
    "with open(train_cm17_tf) as infile:\n",
    "    for line in infile:\n",
    "        image_path, mask_path, x_center, y_center, tf = line.strip('\\n').split(',')[0:5]\n",
    "        pid = os.path.basename(image_path)\n",
    "        mask_name = os.path.basename(mask_path)            \n",
    "        wsi_dict[pid].append([mask_name, x_center, y_center, tf])\n",
    "\n",
    "with open(valid_cm17_tf) as infile:\n",
    "    for line in infile:\n",
    "        image_path, mask_path, x_center, y_center, tf = line.strip('\\n').split(',')[0:5]\n",
    "        pid = os.path.basename(image_path)\n",
    "        mask_name = os.path.basename(mask_path)            \n",
    "        wsi_dict[pid].append([mask_name, x_center, y_center, tf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(wsi_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_dict['Test_071.tif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
